---
title: "ps6 solutions"
output:
  html_document: default
  pdf_document: default
---

# Question 1

## preliminaries
```{r}
# clear workspace
 rm(list=ls())
 # set random seed
set.seed(123)

#Helper library
library(coda)
library(LaplacesDemon)
```

## Step 2: Program the Gibbs sampler
```{r}
#conditional posterior ditribution for theta_J
conditional_post <- function(tao, mu, sigma, n_j, y_j) {
  numer_t = mu / tao^2 + (n_j * mean(y_j)) / sigma^2
  denom_t = (1 / tao^2) + (n_j / sigma^2)
  theta_hat = numer_t / denom_t
  denom_sd = (1 / tao^2) + (n_j / sigma^2)
  sd = sqrt(1 / denom_sd)
  return(rnorm(1, theta_hat, sd))
}

GibssSampler <- function(y1, y2, y3, y4, y5, y6, sampleSize, unkowns, machines) {
  n_1 = length(y1)
  n_2 = length(y2)
  n_3 = length(y3)
  n_4 = length(y4)
  n_5 = length(y5) 
  n_6 = length(y6)
  n = n_1 + n_2 + n_3 + n_4 + n_5 + n_6
  
  params = array(NA, dim = c(sampleSize, length(unkowns)))
  
  previous = unknowns
  for (t in 1:sampleSize) {
    tao = previous[9]
    sig = previous[8]
    mu = previous[7]
    
    #conditional posterior for theta_j = N(theta_hat, Vtheta_j)
    theta_1 = conditional_post(tao, mu, sig, n_1, y1)
    theta_2 = conditional_post(tao, mu, sig, n_2, y2)
    theta_3 = conditional_post(tao, mu, sig, n_3, y3)
    theta_4 = conditional_post(tao, mu, sig, n_4, y4)
    theta_5 = conditional_post(tao, mu, sig, n_5, y5)
    theta_6 = conditional_post(tao, mu, sig, n_6, y6)
    
    theta_inner = c(theta_1, theta_2, theta_3, theta_4, theta_5, theta_6)
    
    #condtional posterior for mu = N(mu_hat, tao^2 / j) w/ mu_hat = 1/j sum(theta j)
    mu_inner = rnorm(1, mean(theta_inner), tao /sqrt(machines))
    
    #conditional posterior for sigma^2
    sample_v1 = sum((y1 - theta_1) ^ 2)
    sample_v2 = sum((y2 - theta_2) ^ 2)
    sample_v3 = sum((y3 - theta_3) ^ 2)
    sample_v4 = sum((y4 - theta_4) ^ 2)
    sample_v5 = sum((y5 - theta_5) ^ 2)
    sample_v6 = sum((y6 - theta_6) ^ 2)
    
    sample_v = (sample_v1 + sample_v2 + sample_v3 + sample_v4 + sample_v5 + sample_v6) / n
    sigma_inner = rinvchisq(1, n, sample_v)
    
    #conditional distribution of tao ^ 2
    sample_t = sum((theta_inner - mu_inner) ^ 2) / (machines - 1)
    tao_inner = rinvchisq(1, machines - 1, sample_t)
    
    #sample parameters
    param_inner = c(theta_1, theta_2, theta_3, theta_4, theta_5, theta_6, 
                  mu_inner, sqrt(sigma_inner), sqrt(tao_inner))
    
    params[t, ] = param_inner
    
    previous = params[t, ]
    
    if (t %% 20000 == 0) {
      paste("Current sample: ", t)
    }
  }
  return(params)
}
```

## step 3: Sampling and diagnostics

```{r}
#Set parameters to pass in
m1 = c(83, 92, 92, 46)
m2 = c(117, 109, 114, 104)
m3 = c(101, 93, 92, 86)
m4 = c(105, 119, 116, 102)
m5 = c(79, 97, 103, 79)
m6 = c(57, 92, 104, 77)
samples = 100000

innit <- function() {
  t1 = sample(m1, 1)
  t2 = sample(m2, 1)
  t3 = sample(m3, 1)
  t4 = sample(m4, 1)
  t5 = sample(m5, 1)
  t6 = sample(m6, 1)
  m = mean(c(m1, m2, m3, m4, m5, m6))
  s = runif(1, 0.1, 5)
  t = runif(1, 0.1, 5)
  return(c(t1, t2, t3, t4, t5, t6, m, s, t))
}

unknowns = innit()
posteriorSamples = GibssSampler(m1, m2, m3, m4, m5, m6, samples, unknowns, 6)
posteriorSamples2 = GibssSampler(m1, m2, m3, m4, m5, m6, samples, unknowns, 6)

#Drop first half
posteriorSamples = posteriorSamples[(samples/2 + 1) : samples, ]
posteriorSamples2 = posteriorSamples2[(samples/2 + 1) : samples, ]

```
Convergences Diag
```{r}
postS = mcmc(posteriorSamples)
postS2 = mcmc(posteriorSamples2)
summary(postS)
summary(postS2)
```
```{r}
autocorr.plot(postS)
autocorr.plot(postS2)
gelman.plot(mcmc.list(postS, postS2))
gelman.diag(mcmc.list(postS, postS2))
```
```{r}
hist(posteriorSamples[,1], xlab="theta_1", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,1]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,2], xlab="theta_2", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,2]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,3], xlab="theta_3", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,3]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,4], xlab="theta_4", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,4]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,5], xlab="theta_5", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,5]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,6], xlab="theta_6", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,6]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,7], xlab="Mu", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,7]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,8], xlab="Sigma", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,8]), col = "red", lwd = 3)
```
```{r}
hist(posteriorSamples[,9], xlab="Tau", breaks = 50, prob=TRUE, main = "")
abline(v = median(posteriorSamples[,9]), col = "red", lwd = 3)
```
Based on the plots we can see that our Markov chain is sampling properly with
our thetas converging to a normal distribution. Also as seen in our 
autocorrelation that our correlations approaches and remains very close to 
zero (might be zero, but plots too small) showing that our samples have 
virtually no correlation. Also, using the Gelman-Robin diagonostic we get 
a psrf of 1 meaning that we are converging to a posterior distribution. 

## step 4: marginal posterior summary statistics
```{r}
library(knitr)
library(kableExtra)

quantiles = c(0.01, 0.05, 0.5, 0.95, 0.99)
#to store the results
parameter_stats = matrix(NA, nrow = 9, ncol = 6)
rownames(parameter_stats) = c("Theta_1", "Theta_2", "Theta_3", "Theta_4", "Theta_5", "Theta_6", "Mu", "Sigma", "Tau")
colnames(parameter_stats) =  c("1%-quantile", "5%-quantile", "Mean", "Median", "95%-quantile", "99%-quantile")

# Calculate statistics for each parameter
for (i in 1:9) {
  parameter_samples <- postS[, i]
  quantile_values <- quantile(parameter_samples, probs = quantiles)
  parameter_stats[i, ] <- c(quantile_values[1], quantile_values[2], mean(parameter_samples),
                            median(parameter_samples), quantile_values[4], quantile_values[5])
}


parameter_df <- as.data.frame(parameter_stats)
table = kable(parameter_df, caption = "Marginal Posterior Distribution Statistics")
kable_styling(table, full_width = FALSE)
```

## step 5: compute P(theta_5 < theta_1)
```{r}
prob = sum(posteriorSamples[, 5] < posteriorSamples[, 1]) / length(posteriorSamples[, 5] < posteriorSamples[, 1])
paste(" 𝑃(𝜃5 < 𝜃1: ", prob)
```

# Question 2

## prelimns
```{r}
library(mvtnorm)
```

## step 1: function to simulate data
```{r}
simulateData <- function(students, books, mu, tau, sigma) {
  mean_affect = rnorm(books, mu, tau)
  scores = rmvnorm(students, mean_affect, sigma * diag(books))
  #indepndent draw hence use for loop
  return(list(mean_affect, scores))
}
```

## step 2: function to estimate posteriors usign Gibbs sampling
```{r}
#conditional posterior ditribution for theta_J
conditional_post <- function(tao, mu, sigma, n_j, y_j) {
  numer_t = mu / tao^2 + (n_j * mean(y_j)) / sigma^2
  denom_t = (1 / tao^2) + (n_j / sigma^2)
  theta_hat = numer_t / denom_t
  denom_sd = (1 / tao^2) + (n_j / sigma^2)
  sd = sqrt(1 / denom_sd)
  return(rnorm(1, theta_hat, sd))
}

generalizedGibbs <- function(unknowns, y, samples) {
  num_thetas = ncol(y)
  n = nrow(y)
  n_sum = n * num_thetas
  
  posteriorSamples = array(NA, dim = c(samples, length(unknowns)))
  previous = unknowns
  
  for (t in 1:samples) {
    #conditional posterior for theta
    mu = previous[num_thetas + 1]
    sig = previous[num_thetas + 2]
    tau = previous[num_thetas + 3]
    
    theta_inner = array(NA, dim = c(num_thetas))
    sample_v = array(NA, dim = c(num_thetas))
    for (j in 1:num_thetas) {
      theta_inner[j] = conditional_post(tau, mu, sig, n, y[ , j])
      sample_v[j] = sum((y[ , j] - theta_inner[j]) ^ 2)
    }
    
    #conditional posterior for mu
    mu_inner = rnorm(1, mean(theta_inner), tau / sqrt(groups))
    
    #conditional posterior for sigma ^ 2
    sample_vt = sum(sample_v) / n_sum
    sigma_inner = rinvchisq(1, n_sum, sample_vt)
    
    #conditional posterior for tau ^ 2
    sample_t = sum((theta_inner - mu_inner) ^ 2) / (groups - 1)
    tau_inner = rinvchisq(1, groups - 1, sample_t)
    
    posteriorSamples[t, ] = c(theta_inner, mu_inner, sqrt(sigma_inner), sqrt(tau_inner))
    previous = posteriorSamples[t, ]
  }
  return(posteriorSamples)
}

posteriorSampler <- function(data) {
  groups = ncol(data)
  general_innit <- function() {
    thetas = array(NA, dim = c(groups))
    for (i in 1:groups) {
      thetas[i] = sample(data[ , i], 1)
    }
    mu = mean(data)
    sig = runif(1, 0.1, 5)
    tau = runif(1, 0.1, 5)
    
    return(c(thetas, mu, sig, tau))
  }
  
  samples = 10000
  unknowns = general_innit()
  postSamples = generalizedGibbs(unknowns, data, samples)
  postSamples = postSamples[(samples/2 + 1) : samples, ]
  return(postSamples)
}
```

## step 3: function to compute expected square errors
```{r}
loss <- function(trueParams, postSamples) {
  npar = length(trueParams)
  true_means = trueParams[1:(npar-3)]
  mu = trueParams[npar-2]
  sig = trueParams[npar-1]
  tao = trueParams[npar]
  
  
  sample_mu = sample(postSamples[, npar-2], 10000, replace=TRUE)
  sample_sig = sample(postSamples[, npar-1], 10000, replace=TRUE)
  sample_tao = sample(postSamples[, npar], 10000, replace=TRUE)
  ese_mu = sum((sample_mu - mu)^2)/10000
  ese_tao = sum((sample_tao - tao)^2)/10000
  ese_sig = sum((sample_sig - sig)^2)/10000
  
  ese_theta = c()
  for (i in 1:length(true_means)) {
    theta = sample(postSamples[,i], 10000, replace=TRUE)
    ese_theta[i] = sum((theta - true_means[i])^2)/10000
  }
  
  return(list(ese_mu, ese_tao, ese_sig, ese_theta))
}
```

## step 4: run simulations and plot results
```{r}
J = c(10, 20, 40, 80)
ndatasets = 10
mu = -10
tao = 100
sig = 25

ese_1 = array(rep(-1, 40), dim = c(10,4))
ese_2 = array(rep(-1, 40), dim = c(10,4))
ese_3 = array(rep(-1, 40), dim = c(10,4))
ese_4 = array(rep(-1, 40), dim = c(10,4))

for (j in 1:length(J)) {
  for (n in 1:ndatasets) {
    data_simulation = simulateData(960/J[j], J[j], mu, tao, sig)
    data = data_simulation[[2]]
    thetas = data_simulation[[1]]
    
    groups = ncol(data)
    postSamples = posteriorSampler(data = data)  
    errors = loss(c(thetas, mu, sig, tao), postSamples)
    
    ese_mu = errors[[1]]
    ese_tao = errors[[2]]
    ese_sig = errors[[3]]
    ese_theta = errors[[4]]
    
    if (j == 1) {
      ese_1[n, ] = c(ese_mu, ese_tao, ese_sig, mean(ese_theta))
    } else if (j == 2) {
      ese_2[n, ] = c(ese_mu, ese_tao, ese_sig, mean(ese_theta))
    } else if (j == 3) {
      ese_3[n, ] = c(ese_mu, ese_tao, ese_sig, mean(ese_theta))
    } else if (j == 4) {
      ese_4[n, ] = c(ese_mu, ese_tao, ese_sig, mean(ese_theta))
    }
  }
}
```
### Plots
```{r}
plot(J, c(mean(ese_1[ , 1]), mean(ese_2[ , 1]), mean(ese_3[ , 1]), mean(ese_4[ , 1])), type = "l", 
     col = "blue", ylab = "MSE Mu", xlab = "Number of Books", main = "Number of Books vs MSE Mu", 
     ylim = c(200, 2500), lty = 2)
lines(J, c(mean(ese_1[ , 1]) - sd(ese_1[ , 1]), mean(ese_2[ , 1]) - sd(ese_2[ , 1]), 
           mean(ese_3[ , 1]) - sd(ese_3[ , 1]), mean(ese_4[ , 1]) - sd(ese_4[ , 1])), 
      col = "purple")
lines(J, c(mean(ese_1[ , 1]) + sd(ese_1[ , 1]), mean(ese_2[ , 1]) + sd(ese_2[ , 1]), 
           mean(ese_3[ , 1]) + sd(ese_3[ , 1]), mean(ese_4[ , 1]) + sd(ese_4[ , 1])), 
      col = "red")
legend("topright", c("Mean", "Mean + StdDev", "Mean - StdDev"), lty = c(1,1,1),
       col = c("blue", "purple", "red"))
```
```{r}
plot(J, c(mean(ese_1[ , 2]), mean(ese_2[ , 2]), mean(ese_3[ , 2]), mean(ese_4[ , 2])), type = "l", 
     col = "blue", ylab = "MSE Sigma", xlab = "Number of Books", main = "Number of Books vs MSE Sigma", 
     ylim = c(0, 2500), lty = 2)
lines(J, c(mean(ese_1[ , 2]) - sd(ese_1[ , 2]), mean(ese_2[ , 2]) - sd(ese_2[ , 2]), 
           mean(ese_3[ , 2]) - sd(ese_3[ , 2]), mean(ese_4[ , 2]) - sd(ese_4[ , 2])), 
      col = "purple")
lines(J, c(mean(ese_1[ , 2]) + sd(ese_1[ , 2]), mean(ese_2[ , 2]) + sd(ese_2[ , 2]), 
           mean(ese_3[ , 2]) + sd(ese_3[ , 2]), mean(ese_4[ , 2]) + sd(ese_4[ , 2])), 
      col = "red")
legend("topright", c("Mean", "Mean + StdDev", "Mean - StdDev"), lty = c(1,1,1),
       col = c("blue", "purple", "red"))
```
```{r}
plot(J, c(mean(ese_1[ , 3]), mean(ese_2[ , 3]), mean(ese_3[ , 3]), mean(ese_4[ , 3])), type = "l", 
     col = "blue", ylab = "MSE Tao", xlab = "Number of Books", main = "Number of Books vs MSE Tao", 
     ylim = c(390, 410), lty = 2)
lines(J, c(mean(ese_1[ , 3]) - sd(ese_1[ , 3]), mean(ese_2[ , 3]) - sd(ese_2[ , 3]), 
           mean(ese_3[ , 3]) - sd(ese_3[ , 3]), mean(ese_4[ , 3]) - sd(ese_4[ , 3])), 
      col = "purple")
lines(J, c(mean(ese_1[ , 3]) + sd(ese_1[ , 3]), mean(ese_2[ , 3]) + sd(ese_2[ , 3]), 
           mean(ese_3[ , 3]) + sd(ese_3[ , 3]), mean(ese_4[ , 3]) + sd(ese_4[ , 3])), 
      col = "red")
legend("topright", c("Mean", "Mean + StdDev", "Mean - StdDev"), lty = c(1,1,1),
       col = c("blue", "purple", "red"))
```
```{r}
plot(J, c(mean(ese_1[ , 4]), mean(ese_2[ , 4]), mean(ese_3[ , 4]), mean(ese_4[ , 4])), type = "l", 
     col = "blue", ylab = "MSE Theta_j", xlab = "Number of Books", main = "Number of Books vs MSE Theta_j", 
     ylim = c(0, 5), lty = 2)
lines(J, c(mean(ese_1[ , 4]) - sd(ese_1[ , 4]), mean(ese_2[ , 4]) - sd(ese_2[ , 4]), 
           mean(ese_3[ , 4]) - sd(ese_3[ , 4]), mean(ese_4[ , 4]) - sd(ese_4[ , 4])), 
      col = "purple")
lines(J, c(mean(ese_1[ , 4]) + sd(ese_1[ , 4]), mean(ese_2[ , 4]) + sd(ese_2[ , 4]), 
           mean(ese_3[ , 4]) + sd(ese_3[ , 4]), mean(ese_4[ , 4]) + sd(ese_4[ , 4])), 
      col = "red")
legend("topright", c("Mean", "Mean + StdDev", "Mean - StdDev"), lty = c(1,1,1),
       col = c("blue", "purple", "red"))
```

## step 5.
We see that all the other mean square errors decrease as the number of books increase
except for our thetas. Therefore, I would pick 80 books as more books leads
to lower mu, sigma, and tao MSE.